import argparse
import json
import time
from pathlib import Path
from typing import List, Dict
import sys
import toml
from tqdm import tqdm

from release_collector import (
    Repository,
    load_processed_repos,
    get_repositories_to_process,
    process_single_repository,
)
from release_analyzer import analyze_repository_releases, ReleaseAnalysis, load_analysis_cache
from pr_analyzer import enhance_release_analysis_with_pr_details, load_pr_analysis_cache

# --- é…ç½®åŠ è½½ ---
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_file = Path(__file__).parent / "config.toml"
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = toml.load(f)
        return config
    except Exception as e:
        print(f"âŒ æ— æ³•åŠ è½½é…ç½®æ–‡ä»¶: {e}")
        sys.exit(1)

CONFIG = load_config()
OUTPUT_DIR = Path(__file__).parent / CONFIG['common']['output_dir']
FINAL_RESULTS_FILE = OUTPUT_DIR / CONFIG['main']['final_results_file']

def setup_output_directory():
    """åˆ›å»ºè¾“å‡ºç›®å½•"""
    OUTPUT_DIR.mkdir(exist_ok=True)
    print(f"âœ… è¾“å‡ºç›®å½•å·²å‡†å¤‡: {OUTPUT_DIR}")

def collect_repositories(use_cache: bool = True, crawl_mode: str = None) -> List[Repository]:
    """æ”¶é›†å’Œå¤„ç†ä»“åº“"""
    print("\nğŸ” === æ­¥éª¤1: æ”¶é›†ä»“åº“ä¿¡æ¯ ===")
    
    # è·å–éœ€è¦å¤„ç†çš„ä»“åº“åˆ—è¡¨å’Œå·²å¤„ç†çš„ä»“åº“
    pre_filtered_repos, processed_repos = get_repositories_to_process(use_cache, crawl_mode)
    
    if not pre_filtered_repos:
        print("âŒ æ²¡æœ‰ä»“åº“é€šè¿‡åˆæ­¥ç­›é€‰")
        # å¦‚æœæ²¡æœ‰æ–°ä»“åº“é€šè¿‡ç­›é€‰ï¼Œä½†æœ‰å·²å¤„ç†çš„ä»“åº“ï¼Œè¿”å›å·²å¤„ç†çš„ä»“åº“
        if processed_repos:
            print(f"ğŸ“‚ è¿”å› {len(processed_repos)} ä¸ªå·²å¤„ç†çš„ä»“åº“")
            return list(processed_repos.values())
        return []
        
    print(f"âœ… {len(pre_filtered_repos)} ä¸ªä»“åº“é€šè¿‡åˆæ­¥ç­›é€‰")
    
    # å¤„ç†æ¯ä¸ªä»“åº“
    final_repositories = []
    
    # å…ˆæ·»åŠ å·²å¤„ç†çš„ä»“åº“
    if processed_repos:
        final_repositories.extend(processed_repos.values())
        print(f"ğŸ“‚ åŠ è½½äº† {len(processed_repos)} ä¸ªå·²å¤„ç†çš„ä»“åº“")
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºå¤„ç†è¿›åº¦
    with tqdm(pre_filtered_repos, desc="å¤„ç†ä»“åº“", unit="repo") as pbar:
        for repo in pbar:
            repo_name = repo['full_name']
            pbar.set_description(f"å¤„ç†: {repo_name}")
            
            try:
                repository = process_single_repository(repo, use_cache)
                final_repositories.append(repository)
                pbar.write(f"  âœ… {repo_name}: å¤„ç†å®Œæˆ")
            except Exception as e:
                pbar.write(f"  âŒ {repo_name}: {str(e)}")
                continue
    
    print(f"\nâœ… æ”¶é›†é˜¶æ®µå®Œæˆï¼Œå…±å¤„ç† {len(final_repositories)} ä¸ªä»“åº“")
    return final_repositories

def analyze_releases(repositories: List[Repository]) -> List[ReleaseAnalysis]:
    """åˆ†ææ‰€æœ‰ä»“åº“çš„release"""
    print("\nğŸ“Š === æ­¥éª¤2: åˆ†æReleaseåŠŸèƒ½ ===")
    
    all_analyses = []
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºåˆ†æè¿›åº¦
    with tqdm(repositories, desc="åˆ†æä»“åº“", unit="repo") as pbar:
        for repository in pbar:
            pbar.set_description(f"åˆ†æ: {repository.full_name}")
            analyses = analyze_repository_releases(repository)
            all_analyses.extend(analyses)
            
            # ç»Ÿè®¡å½“å‰ä»“åº“çš„åˆ†æç»“æœ
            total_new_features = sum(len(a.new_features) for a in analyses)
            total_improvements = sum(len(a.improvements) for a in analyses)
            total_bug_fixes = sum(len(a.bug_fixes) for a in analyses)
            
            pbar.write(f"  âœ… {repository.full_name}: æ–°åŠŸèƒ½({total_new_features}) æ”¹è¿›({total_improvements}) ä¿®å¤({total_bug_fixes})")
    
    # ç»Ÿè®¡æ€»ä½“ç»“æœ
    total_new_features = sum(len(a.new_features) for a in all_analyses)
    total_improvements = sum(len(a.improvements) for a in all_analyses)
    total_bug_fixes = sum(len(a.bug_fixes) for a in all_analyses)
    
    print(f"\nâœ… Releaseåˆ†æå®Œæˆ!")
    print(f"  - å…±åˆ†æ {len(all_analyses)} ä¸ªrelease")
    print(f"  - æ–°å¢åŠŸèƒ½: {total_new_features}")
    print(f"  - åŠŸèƒ½æ”¹è¿›: {total_improvements}")
    print(f"  - æ¼æ´ä¿®å¤: {total_bug_fixes}")
    
    return all_analyses

def enhance_with_pr_analysis(release_analyses: List[ReleaseAnalysis]) -> List[Dict]:
    """å¢å¼ºPRåˆ†æ"""
    print("\nğŸ”§ === æ­¥éª¤3: å¢å¼ºPRè¯¦ç»†åˆ†æ ===")
    
    enhanced_results = []
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºPRåˆ†æè¿›åº¦
    with tqdm(release_analyses, desc="åˆ†æPR", unit="release") as pbar:
        for analysis in pbar:
            pbar.set_description(f"åˆ†æ: {analysis.repo_name}-{analysis.tag_name}")
            
            # åªå¯¹æ–°åŠŸèƒ½è¿›è¡ŒPRè¯¦ç»†åˆ†æ
            enhanced_features = enhance_release_analysis_with_pr_details(analysis)
            
            if enhanced_features:
                result = {
                    'repository': analysis.repo_name,
                    'release': analysis.tag_name,
                    'analyzed_at': analysis.analyzed_at,
                    'enhanced_new_features': [ef.to_dict() for ef in enhanced_features],
                    'original_analysis': analysis.to_dict()
                }
                enhanced_results.append(result)
                pbar.write(f"  âœ… {analysis.repo_name}-{analysis.tag_name}: åˆ†æäº† {len(enhanced_features)} ä¸ªPR")
            else:
                pbar.write(f"  âš ï¸ {analysis.repo_name}-{analysis.tag_name}: æ²¡æœ‰å¯åˆ†æçš„æ–°åŠŸèƒ½")
    
    print(f"\nâœ… PRè¯¦ç»†åˆ†æå®Œæˆï¼Œå…±åˆ†æ {len(enhanced_results)} ä¸ªRelease")
    return enhanced_results

def save_final_results(enhanced_results: List[Dict]):
    """ä¿å­˜æœ€ç»ˆç»“æœ"""
    print("\nğŸ’¾ === æ­¥éª¤4: ä¿å­˜æœ€ç»ˆç»“æœ ===")
    
    final_output = {
        'metadata': {
            'generated_at': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_repositories': len(set(r['repository'] for r in enhanced_results)),
            'total_releases': len(enhanced_results),
            'total_enhanced_features': sum(len(r['enhanced_new_features']) for r in enhanced_results)
        },
        'results': enhanced_results
    }
    
    try:
        with open(FINAL_RESULTS_FILE, 'w', encoding='utf-8') as f:
            json.dump(final_output, f, indent=2, ensure_ascii=False)
        print(f"âœ… æœ€ç»ˆç»“æœå·²ä¿å­˜åˆ°: {FINAL_RESULTS_FILE}")
        
        # æ‰“å°ç»“æœæ‘˜è¦
        print(f"\nğŸ“ˆ === æœ€ç»ˆç»Ÿè®¡ ===")
        print(f"  - åˆ†æä»“åº“æ•°: {final_output['metadata']['total_repositories']}")
        print(f"  - åˆ†æreleaseæ•°: {final_output['metadata']['total_releases']}")
        print(f"  - å¢å¼ºåŠŸèƒ½æ•°: {final_output['metadata']['total_enhanced_features']}")
        
    except Exception as e:
        print(f"âŒ ä¿å­˜ç»“æœå¤±è´¥: {e}")

def print_sample_results(enhanced_results: List[Dict], limit: int = 5):
    """æ‰“å°ç¤ºä¾‹ç»“æœ"""
    # å¦‚æœæ²¡æœ‰æŒ‡å®šé™åˆ¶ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼
    if limit is None:
        limit = CONFIG['main']['sample_results_limit']
        
    print(f"\nğŸ¯ === ç¤ºä¾‹ç»“æœé¢„è§ˆ (å‰{limit}ä¸ª) ===")
    
    for i, result in enumerate(enhanced_results[:limit]):
        print(f"\n--- ç¤ºä¾‹ {i+1}: {result['repository']} - {result['release']} ---")
        
        enhanced_features = result['enhanced_new_features']
        for j, feature in enumerate(enhanced_features[:2]):  # æ¯ä¸ªreleaseåªæ˜¾ç¤ºå‰2ä¸ªåŠŸèƒ½
            print(f"  åŠŸèƒ½ {j+1}: {feature['description'][:100]}...")
            if feature['pr_analyses']:
                pr_count = len(feature['pr_analyses'])
                print(f"    - å…³è”PRæ•°: {pr_count}")
                print(f"    - è¯¦ç»†æè¿°: {feature['feature_detailed_description'][:150]}...")

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='GitHubä»“åº“releaseå’ŒPRåˆ†æå·¥å…·')
    parser.add_argument('--no-cache', action='store_true',
                       help='ä¸ä½¿ç”¨ç¼“å­˜ï¼Œé‡æ–°å¤„ç†æ‰€æœ‰æ•°æ®')
    parser.add_argument('--collect-only', action='store_true',
                       help='åªæ‰§è¡Œä»“åº“æ”¶é›†ï¼Œä¸è¿›è¡Œåç»­åˆ†æ')
    parser.add_argument('--analyze-only', action='store_true',
                       help='åªæ‰§è¡Œreleaseåˆ†æï¼Œè·³è¿‡ä»“åº“æ”¶é›†')
    parser.add_argument('--enhance-only', action='store_true',
                       help='åªæ‰§è¡ŒPRå¢å¼ºåˆ†æï¼Œè·³è¿‡å‰é¢æ­¥éª¤')
    parser.add_argument('--crawl-mode', choices=['stars', 'specified'], default='specified',
                       help='é€‰æ‹©çˆ¬å–æ¨¡å¼: stars(æŒ‰staræ•°ç­›é€‰) æˆ– specified(ä½¿ç”¨æŒ‡å®šä»“åº“åˆ—è¡¨)')
    
    args = parser.parse_args()
    
    print("ğŸš€ å¼€å§‹GitHubä»“åº“åˆ†ææµç¨‹")
    print("=" * 50)
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    setup_output_directory()
    
    use_cache = not args.no_cache
    enhanced_results = []
    
    try:
        if not args.analyze_only and not args.enhance_only:
            # æ­¥éª¤1: æ”¶é›†ä»“åº“
            repositories = collect_repositories(use_cache=use_cache, crawl_mode=args.crawl_mode)
            
            if not repositories:
                print("âŒ æ²¡æœ‰æ”¶é›†åˆ°æœ‰æ•ˆä»“åº“ï¼Œç¨‹åºç»“æŸ")
                return
                
            if args.collect_only:
                print(f"âœ… ä»…æ”¶é›†æ¨¡å¼å®Œæˆï¼Œå…±æ”¶é›† {len(repositories)} ä¸ªä»“åº“")
                return
        else:
            # ä»ç¼“å­˜åŠ è½½ä»“åº“æ•°æ®
            processed_repos = load_processed_repos()
            repositories = list(processed_repos.values())
            print(f"ğŸ“‚ ä»ç¼“å­˜åŠ è½½äº† {len(repositories)} ä¸ªä»“åº“")

        if not args.enhance_only:
            # æ­¥éª¤2: åˆ†ærelease
            release_analyses = analyze_releases(repositories)
            
            if not release_analyses:
                print("âŒ æ²¡æœ‰åˆ†æåˆ°æœ‰æ•ˆreleaseï¼Œç¨‹åºç»“æŸ")
                return
                
            if args.analyze_only:
                print("âœ… ä»…åˆ†ææ¨¡å¼å®Œæˆ")
                return
        else:
            cached_analyses = load_analysis_cache()
            release_analyses = list(cached_analyses.values())
            print(f"ğŸ“‚ ä»ç¼“å­˜åŠ è½½äº† {len(release_analyses)} ä¸ªreleaseåˆ†æ")
        
        # æ­¥éª¤3: å¢å¼ºPRåˆ†æ
        cached_pr_analysis = load_pr_analysis_cache()
        pr_analysis = list(cached_pr_analysis.values())
        print(f"ğŸ“‚ ä»ç¼“å­˜åŠ è½½äº† {len(pr_analysis)} ä¸ªPRåˆ†æ")

        enhanced_results = enhance_with_pr_analysis(release_analyses)

        if enhanced_results:
            save_final_results(enhanced_results)
            print_sample_results(enhanced_results)
        
        print(f"\nğŸ‰ å®Œæ•´åˆ†ææµç¨‹å®Œæˆï¼")
        
    except KeyboardInterrupt:
        print("\nâš ï¸ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
        sys.exit(1)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()