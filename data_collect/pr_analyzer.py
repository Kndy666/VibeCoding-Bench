import ast
import re
import json
import time
import toml
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass, asdict
import openai
from pathlib import Path
import requests
import base64
from utils import is_test_file
from tqdm import tqdm

# --- é…ç½®åŠ è½½ ---
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_file = Path(__file__).parent / "config.toml"
    with open(config_file, 'r', encoding='utf-8') as f:
        return toml.load(f)

CONFIG = load_config()

# --- é…ç½®åŒº ---
GITHUB_TOKEN = CONFIG['common']['github_token']
OPENAI_API_KEY = CONFIG['common']['openai_api_key']
OPENAI_MODEL = CONFIG['common']['openai_model']

# ç¼“å­˜æ–‡ä»¶
PR_ANALYSIS_CACHE_FILE = Path(__file__).parent / CONFIG['common']['output_dir'] / CONFIG['pr_analyzer']['pr_analysis_cache_file']

# GitHub API åŸºç¡€URL
GITHUB_API_BASE = CONFIG['common']['github_api_base']

HEADERS = {
    'Authorization': f'token {GITHUB_TOKEN}',
    'Accept': 'application/vnd.github.v3+json'
}

# --- æ•°æ®ç±»å®šä¹‰ ---

@dataclass
class TestFile:
    """è¡¨ç¤ºä¸€ä¸ªæµ‹è¯•æ–‡ä»¶"""
    path: str
    content: str
    size: int
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'TestFile':
        return cls(**data)

@dataclass
class FileChange:
    """è¡¨ç¤ºä¸€ä¸ªæ–‡ä»¶çš„å˜æ›´ä¿¡æ¯"""
    filename: str
    status: str  # 'added', 'removed', 'modified', 'renamed'
    additions: int
    deletions: int
    changes: int
    patch: Optional[str] = None  # diff å†…å®¹
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'FileChange':
        return cls(**data)

@dataclass
class Commit:
    """è¡¨ç¤ºä¸€ä¸ªGitæäº¤"""
    sha: str
    message: str
    date: str
    author: str
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'Commit':
        return cls(**data)

@dataclass
class PRAnalysis:
    """è¡¨ç¤ºä¸€ä¸ª PR çš„è¯¦ç»†åˆ†æç»“æœ"""
    pr_number: str
    repo_name: str
    title: str
    description: str
    state: str  # 'open', 'closed', 'merged'
    merged: bool
    base_commit: Commit  # PRå‰çš„commitä¿¡æ¯
    head_commit: Commit  # PRåçš„commitä¿¡æ¯
    file_changes: List[FileChange]
    detailed_description: str  # LLM åŸºäºæ–‡ä»¶å˜æ›´ç”Ÿæˆçš„è¯¦ç»†æè¿°
    has_tests: bool  # æ˜¯å¦æ‰¾åˆ°ç›¸å…³æµ‹è¯•
    test_files: List[str]  # æµ‹è¯•æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    only_modified_existing_functions: bool # æ˜¯å¦åªä¿®æ”¹äº†å·²æœ‰å‡½æ•°
    non_test_files: List[str] # éæµ‹è¯•æ–‡ä»¶è·¯å¾„åˆ—è¡¨
    analyzed_at: str
    
    def to_dict(self) -> Dict:
        return {
            'pr_number': self.pr_number,
            'repo_name': self.repo_name,
            'title': self.title,
            'description': self.description,
            'state': self.state,
            'merged': self.merged,
            'base_commit': self.base_commit.to_dict(),
            'head_commit': self.head_commit.to_dict(),
            'file_changes': [fc.to_dict() for fc in self.file_changes],
            'detailed_description': self.detailed_description,
            'has_tests': self.has_tests,
            'test_files': self.test_files,
            'only_modified_existing_functions': self.only_modified_existing_functions,
            'non_test_files': self.non_test_files or [],
            'analyzed_at': self.analyzed_at
        }
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'PRAnalysis':
        return cls(
            pr_number=data['pr_number'],
            repo_name=data['repo_name'],
            title=data['title'],
            description=data['description'],
            state=data['state'],
            merged=data['merged'],
            base_commit=Commit.from_dict(data.get('base_commit', {})),
            head_commit=Commit.from_dict(data.get('head_commit', {})),
            file_changes=[FileChange.from_dict(fc) for fc in data.get('file_changes', [])],
            detailed_description=data.get('detailed_description', ''),
            has_tests=data.get('has_tests', False),
            test_files=data.get('test_files', []),
            only_modified_existing_functions=data.get('only_modified_existing_functions', True),
            non_test_files=data.get('non_test_files', []),
            analyzed_at=data.get('analyzed_at', '')
        )

@dataclass
class EnhancedFeature:
    """å¢å¼ºçš„åŠŸèƒ½å¯¹è±¡ï¼ŒåŒ…å« PR è¯¦ç»†åˆ†æ"""
    feature_type: str
    description: str
    pr_analyses: List[PRAnalysis]
    feature_detailed_description: str  # åŸºäºæ‰€æœ‰PRåˆ†æçš„æ•´ä½“è¯¦ç»†æè¿°
    
    def to_dict(self) -> Dict:
        return {
            'feature_type': self.feature_type,
            'description': self.description,
            'pr_analyses': [pr.to_dict() for pr in self.pr_analyses],
            'feature_detailed_description': self.feature_detailed_description
        }
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'EnhancedFeature':
        return cls(
            feature_type=data['feature_type'],
            description=data['description'],
            pr_analyses=[PRAnalysis.from_dict(pr) for pr in data.get('pr_analyses', [])],
            feature_detailed_description=data.get('feature_detailed_description', '')
        )

# --- ç¼“å­˜ç®¡ç† ---

def load_pr_analysis_cache() -> Dict[str, PRAnalysis]:
    """åŠ è½½ PR åˆ†æç¼“å­˜"""
    if PR_ANALYSIS_CACHE_FILE.exists():
        try:
            with open(PR_ANALYSIS_CACHE_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                cache = {}
                for key, pr_data in data.items():
                    cache[key] = PRAnalysis.from_dict(pr_data)
                print(f"âœ… ä»ç¼“å­˜åŠ è½½äº† {len(cache)} ä¸ªPRåˆ†æç»“æœ")
                return cache
        except Exception as e:
            print(f"âš ï¸ åŠ è½½PRåˆ†æç¼“å­˜å¤±è´¥: {e}")
            return {}
    return {}

def save_pr_analysis_to_cache(analysis: PRAnalysis):
    """ä¿å­˜ PR åˆ†æç»“æœåˆ°ç¼“å­˜"""
    cache = {}
    if PR_ANALYSIS_CACHE_FILE.exists():
        try:
            with open(PR_ANALYSIS_CACHE_FILE, 'r', encoding='utf-8') as f:
                cache = json.load(f)
        except:
            pass
    
    cache_key = f"{analysis.repo_name}#{analysis.pr_number}"
    cache[cache_key] = analysis.to_dict()
    
    try:
        with open(PR_ANALYSIS_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ å·²ä¿å­˜ PR#{analysis.pr_number} çš„åˆ†æç»“æœåˆ°ç¼“å­˜")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜PRåˆ†æç¼“å­˜å¤±è´¥: {e}")

# --- GitHub API å‡½æ•° ---

def extract_pr_number_from_url(pr_url: str) -> Optional[str]:
    """ä» PR URL ä¸­æå– PR ç¼–å·"""
    match = re.search(r'/pull/(\d+)', pr_url)
    return match.group(1) if match else None

def get_pr_info(repo_name: str, pr_number: str) -> Optional[Dict]:
    """è·å– PR åŸºæœ¬ä¿¡æ¯"""
    url = f"{GITHUB_API_BASE}/repos/{repo_name}/pulls/{pr_number}"
    
    try:
        response = requests.get(url, headers=HEADERS)
        if response.status_code == 200:
            return response.json()
        else:
            print(f"âš ï¸ è·å–PR#{pr_number}ä¿¡æ¯å¤±è´¥: {response.status_code}")
            return None
    except Exception as e:
        print(f"âš ï¸ è·å–PR#{pr_number}ä¿¡æ¯å¼‚å¸¸: {e}")
        return None

def get_pr_files(repo_name: str, pr_number: str) -> List[FileChange]:
    """è·å– PR çš„æ–‡ä»¶å˜æ›´ä¿¡æ¯"""
    url = f"{GITHUB_API_BASE}/repos/{repo_name}/pulls/{pr_number}/files"
    
    try:
        response = requests.get(url, headers=HEADERS)
        if response.status_code == 200:
            files_data = response.json()
            file_changes = []
            
            for file_data in files_data:
                file_change = FileChange(
                    filename=file_data.get('filename', ''),
                    status=file_data.get('status', ''),
                    additions=file_data.get('additions', 0),
                    deletions=file_data.get('deletions', 0),
                    changes=file_data.get('changes', 0),
                    patch=file_data.get('patch', '')
                )
                file_changes.append(file_change)
            
            return file_changes
        else:
            print(f"âš ï¸ è·å–PR#{pr_number}æ–‡ä»¶å˜æ›´å¤±è´¥: {response.status_code}")
            return []
    except Exception as e:
        print(f"âš ï¸ è·å–PR#{pr_number}æ–‡ä»¶å˜æ›´å¼‚å¸¸: {e}")
        return []

def get_file_content(repo_name: str, file_path: str, ref: str) -> Optional[str]:
    """è·å–æ–‡ä»¶å†…å®¹"""
    try:
        url = f"{GITHUB_API_BASE}/repos/{repo_name}/contents/{file_path}"
        time.sleep(0.3)
        response = requests.get(url, headers=HEADERS, params={'ref': ref})
        
        if response.status_code == 200:
            content_data = response.json()
            if content_data.get('encoding') == 'base64':
                content = base64.b64decode(content_data['content']).decode('utf-8', errors='ignore')
                return content
    except Exception as e:
        print(f"    - è·å–æ–‡ä»¶å†…å®¹å¤±è´¥ {file_path}: {e}")
    
    return None

def get_commit_info(repo_name: str, commit_sha: str) -> Optional[Commit]:
    """è·å–å•ä¸ªæäº¤çš„è¯¦ç»†ä¿¡æ¯"""
    url = f"{GITHUB_API_BASE}/repos/{repo_name}/commits/{commit_sha}"
    
    try:
        response = requests.get(url, headers=HEADERS)
        if response.status_code == 200:
            commit_data = response.json()
            return Commit(
                sha=commit_data.get('sha', ''),
                message=commit_data.get('commit', {}).get('message', ''),
                date=commit_data.get('commit', {}).get('author', {}).get('date', ''),
                author=commit_data.get('commit', {}).get('author', {}).get('name', '')
            )
        else:
            print(f"âš ï¸ è·å–æäº¤{commit_sha[:8]}ä¿¡æ¯å¤±è´¥: {response.status_code}")
            return None
    except Exception as e:
        print(f"âš ï¸ è·å–æäº¤{commit_sha[:8]}ä¿¡æ¯å¼‚å¸¸: {e}")
        return None

def extract_definitions(content: str) -> List[str]:
    """ä»Pythonä»£ç å†…å®¹ä¸­æå–å‡½æ•°å’Œç±»å®šä¹‰ï¼ŒåŒ…æ‹¬åµŒå¥—å…³ç³»"""
    if not content:
        return []
    
    try:
        # è§£æä»£ç ä¸ºAST
        tree = ast.parse(content)
        
        # å­˜å‚¨æ‰€æœ‰å®šä¹‰ï¼ˆåŒ…æ‹¬å®ƒä»¬çš„è·¯å¾„ï¼‰
        definitions = []
        
        def visit_node(node, path=""):
            """é€’å½’è®¿é—®èŠ‚ç‚¹å¹¶æ”¶é›†å®šä¹‰ä¿¡æ¯"""
            if isinstance(node, ast.FunctionDef):
                full_name = f"{path}.{node.name}" if path else node.name
                definitions.append(full_name)
                
                # é€’å½’è®¿é—®å‡½æ•°ä½“å†…çš„å®šä¹‰
                for child in node.body:
                    visit_node(child, full_name)
                    
            elif isinstance(node, ast.ClassDef):
                full_name = f"{path}.{node.name}" if path else node.name
                definitions.append(full_name)
                
                # é€’å½’è®¿é—®ç±»ä½“å†…çš„å®šä¹‰
                for child in node.body:
                    visit_node(child, full_name)
            
            elif isinstance(node, ast.Module):
                # æ¨¡å—çš„é¡¶çº§èŠ‚ç‚¹
                for child in node.body:
                    visit_node(child)
        
        # å¼€å§‹è®¿é—®
        visit_node(tree)
        return definitions
        
    except SyntaxError:
        print(f"    - âš ï¸ è§£æä»£ç å‡ºé”™ï¼Œå¯èƒ½åŒ…å«è¯­æ³•é”™è¯¯")
        return []

def analyze_function_changes(before_content: str, after_content: str) -> Tuple[bool, List[str], List[str]]:
    """åˆ†æå‡½æ•°å˜æ›´ï¼Œè¿”å›æ˜¯å¦åªä¿®æ”¹äº†å·²æœ‰å‡½æ•°åŠæ–°å¢/åˆ é™¤çš„å‡½æ•°åˆ—è¡¨"""
    
    # åˆ†ææ–‡ä»¶å†…å®¹å˜æ›´ï¼ˆä½¿ç”¨å®Œæ•´è·¯å¾„åŒºåˆ†åµŒå¥—å…³ç³»ï¼‰
    before_definitions = set(extract_definitions(before_content or ""))
    after_definitions = set(extract_definitions(after_content or ""))
    
    # æ‰¾å‡ºæ–°å¢å’Œåˆ é™¤çš„å®šä¹‰ï¼ˆåŒ…æ‹¬å‡½æ•°å’Œç±»ï¼‰
    new_definitions = list(after_definitions - before_definitions)
    deleted_definitions = list(before_definitions - after_definitions)
    
    # åˆ¤æ–­æ˜¯å¦åªä¿®æ”¹äº†å·²æœ‰å‡½æ•°å’Œç±»
    only_modified = len(new_definitions) == 0 and len(deleted_definitions) == 0
    
    return only_modified, new_definitions, deleted_definitions

# --- LLM åˆ†æ ---

def generate_detailed_description_with_llm(
    feature_description: str,
    pr_info: Dict,
    file_changes: List[FileChange]
) -> Optional[str]:
    """ä½¿ç”¨ LLM åŸºäºæ–‡ä»¶å˜æ›´ç”Ÿæˆè¯¦ç»†çš„åŠŸèƒ½æè¿°"""
    
    client = openai.OpenAI(api_key=OPENAI_API_KEY, base_url="https://api.deepseek.com")
    
    # è¿‡æ»¤æ‰æµ‹è¯•æ–‡ä»¶ï¼Œåªæ„å»ºéæµ‹è¯•æ–‡ä»¶çš„æ‘˜è¦
    non_test_file_changes = [fc for fc in file_changes if not is_test_file(fc.filename)]
    
    # ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°
    max_files = CONFIG['pr_analyzer']['max_files_in_summary']
    max_patch_length = CONFIG['pr_analyzer']['max_patch_length']
    max_patch_preview = CONFIG['pr_analyzer']['max_patch_preview_length']
    
    # æ„å»ºæ–‡ä»¶å˜æ›´æ‘˜è¦
    files_summary = []
    for fc in non_test_file_changes[:max_files]:  # ä½¿ç”¨é…ç½®çš„æ–‡ä»¶æ•°é‡é™åˆ¶
        summary = f"- {fc.filename} ({fc.status}): +{fc.additions}/-{fc.deletions}"
        if fc.patch and len(fc.patch) < max_patch_length:  # ä½¿ç”¨é…ç½®çš„patché•¿åº¦é™åˆ¶
            summary += f"\n  {fc.patch[:max_patch_preview]}..."  # ä½¿ç”¨é…ç½®çš„é¢„è§ˆé•¿åº¦
        files_summary.append(summary)
    
    files_text = "\n".join(files_summary)
    if len(non_test_file_changes) > max_files:
        files_text += f"\n... and {len(non_test_file_changes) - max_files} more files"
    
    # å¦‚æœæ²¡æœ‰éæµ‹è¯•æ–‡ä»¶ï¼Œæä¾›æç¤ºä¿¡æ¯
    if not non_test_file_changes:
        files_text = "No files were modified in this PR."
    
    prompt = f"""
You need to create a user-focused description for this software update. Analyze the PR information and code changes to understand what this means for users.

Original Feature Description: {feature_description}

PR Title: {pr_info.get('title', '')}
PR Description: {pr_info.get('body', '')}

File Changes:
{files_text}

Your description must:
- Start with "I want to" and focus on user needs and capabilities
- Explain what users can now accomplish with this update
- Highlight the benefits and value this brings to user experience
- Show how this functionality helps users in their workflow
- Avoid technical implementation details
- Use existing PR context when valuable, but enhance it with code insights
- Provide a cohesive explanation of why this update matters to users

Write directly from the user's perspective about what they can accomplish and what value they receive.
"""

    try:
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "You help users understand software updates by writing clear, user-focused descriptions. Always start responses with 'I want to' and explain what users can accomplish and the value they receive. Focus on practical benefits rather than technical details."},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=1000
        )

        content = response.choices[0].message.content
        return content if content is not None else feature_description
        
    except Exception as e:
        print(f"âš ï¸ LLMç”Ÿæˆè¯¦ç»†æè¿°å¤±è´¥: {e}")
        return feature_description

def generate_feature_detailed_description(
    feature_description: str,
    feature_type: str,
    pr_analyses: List[PRAnalysis]
) -> Optional[str]:
    """åŸºäºå¤šä¸ª PR çš„è¯¦ç»†åˆ†æï¼Œä¸ºæ•´ä¸ª feature ç”Ÿæˆè¯¦ç»†æè¿°"""
    
    client = openai.OpenAI(api_key=OPENAI_API_KEY, base_url="https://api.deepseek.com")
    
    # æ„å»ºæ‰€æœ‰PRçš„è¯¦ç»†ä¿¡æ¯æ‘˜è¦
    pr_summaries = []
    for pr in pr_analyses:
        summary = f"""
PR #{pr.pr_number}: {pr.title}
- Status: {pr.state} (merged: {pr.merged})
- Files changed: {len(pr.file_changes)}
- User benefits: {pr.detailed_description}
"""
        pr_summaries.append(summary)
    
    prs_text = "\n".join(pr_summaries)
    
    prompt = f"""
You need to create a comprehensive user-focused description for this complete feature. Analyze all the related pull requests to understand what this feature enables users to do.

Feature Type: {feature_type}
Original Description: {feature_description}

Associated Pull Requests:
{prs_text}

Your description must:
- Start with "I want to" and focus on what users can accomplish
- Explain the specific capabilities this complete feature provides
- Show how all related changes work together to benefit users
- Highlight what new things users can now do that they couldn't before
- Demonstrate how this improves user workflow and productivity
- Explain the practical value users receive from this feature
- Combine insights from all PRs to show the complete picture
- Focus on user benefits rather than technical implementation
- Explain why this feature matters to users in real-world usage

Write directly from the user's perspective about what they can accomplish and the complete value they receive from this feature.
"""

    try:
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "You help users understand software features by writing comprehensive, user-focused descriptions. Always start responses with 'I want to' and explain what users can accomplish and the practical value they receive. Synthesize information from multiple sources to show complete user benefits."},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=1000
        )
        
        content = response.choices[0].message.content
        return content if content is not None else feature_description
        
    except Exception as e:
        print(f"âš ï¸ LLMç”Ÿæˆfeatureè¯¦ç»†æè¿°å¤±è´¥: {e}")
        return None

# --- ä¸»è¦åŠŸèƒ½å‡½æ•° ---

def analyze_pr(repo_name: str, pr_url: str, feature_description: str, use_cache: bool = True) -> Optional[PRAnalysis]:
    """åˆ†æå•ä¸ª PR"""
    pr_number = extract_pr_number_from_url(pr_url)
    if not pr_number:
        print(f"âš ï¸ æ— æ³•ä»URLä¸­æå–PRç¼–å·: {pr_url}")
        return None
    
    cache_key = f"{repo_name}#{pr_number}"
    
    # æ£€æŸ¥ç¼“å­˜
    if use_cache:
        cache = load_pr_analysis_cache()
        if cache_key in cache:
            print(f"  > ğŸ”„ ä»ç¼“å­˜åŠ è½½ PR#{pr_number} çš„åˆ†æç»“æœ")
            return cache[cache_key]
    
    print(f"  > ğŸ” æ­£åœ¨åˆ†æ PR#{pr_number}...")
    
    # è·å– PR åŸºæœ¬ä¿¡æ¯
    pr_info = get_pr_info(repo_name, pr_number)
    if not pr_info:
        return None
    
    # è·å–æ–‡ä»¶å˜æ›´
    file_changes = get_pr_files(repo_name, pr_number)
    
    # è·å–è¯¦ç»†çš„ Commit ä¿¡æ¯
    base_sha = pr_info['base']['sha']
    head_sha = pr_info['head']['sha']
    
    base_commit = get_commit_info(repo_name, base_sha)
    head_commit = get_commit_info(repo_name, head_sha)
    
    # å¦‚æœæ— æ³•è·å–æäº¤ä¿¡æ¯ï¼Œåˆ›å»ºåŸºæœ¬çš„ Commit å¯¹è±¡
    if not base_commit:
        base_commit = Commit(sha=base_sha, message='', date='', author='')
    if not head_commit:
        head_commit = Commit(sha=head_sha, message='', date='', author='')
    
    # è·å–è¯¦ç»†çš„ Commit ä¿¡æ¯
    base_commit = get_commit_info(repo_name, base_sha)
    head_commit = get_commit_info(repo_name, head_sha)
    
    # å¦‚æœæ— æ³•è·å–æäº¤ä¿¡æ¯ï¼Œåˆ›å»ºåŸºæœ¬çš„ Commit å¯¹è±¡
    if not base_commit:
        base_commit = Commit(sha=base_commit.sha, message='', date='', author='')
    if not head_commit:
        head_commit = Commit(sha=head_commit.sha, message='', date='', author='')

    test_files = []
    non_test_files = []
    detailed_description = None
    only_modified_existing_functions = True

    for file_data in file_changes:
        file_path = file_data.filename
        
        if is_test_file(file_path):
            test_files.append(file_path)
            print(f"    - æ‰¾åˆ°æµ‹è¯•æ–‡ä»¶: {file_path}")
        elif file_path.endswith('.py'):  # åªåˆ†æPythonæ–‡ä»¶
            non_test_files.append(file_path)
            
            # æ£€æŸ¥å‡½æ•°å˜æ›´
            status = file_data.status
            
            # å¯¹äºæ–°å¢æˆ–åˆ é™¤çš„æ–‡ä»¶ï¼Œåªè¦å®ƒä»¬åŒ…å«å‡½æ•°å®šä¹‰ï¼Œå°±ä¸æ»¡è¶³æ¡ä»¶
            if status == 'added' or status == 'removed':
                content = get_file_content(repo_name, file_path, head_commit.sha if status == 'added' else base_commit.sha)
                if content and extract_definitions(content):
                    print(f"    - âš ï¸ å‘ç°{status}æ–‡ä»¶å«æœ‰å‡½æ•°å®šä¹‰: {file_path}")
                    only_modified_existing_functions = False
                    break
            
            # å¯¹äºä¿®æ”¹çš„æ–‡ä»¶ï¼Œéœ€è¦æ¯”è¾ƒä¿®æ”¹å‰åçš„å‡½æ•°å®šä¹‰
            elif status == 'modified':
                before_content = get_file_content(repo_name, file_path, base_commit.sha)
                after_content = get_file_content(repo_name, file_path, head_commit.sha)

                # å¤„ç†æ–‡ä»¶å†…å®¹å¯èƒ½ä¸ºNoneçš„æƒ…å†µ
                if before_content is not None and after_content is not None:
                    only_modified, new_funcs, deleted_funcs = analyze_function_changes(before_content, after_content)
                
                    if not only_modified:
                        print(f"    - âš ï¸ æ–‡ä»¶ä¿®æ”¹å«æœ‰å‡½æ•°æ–°å¢æˆ–åˆ é™¤: {file_path}")
                        if new_funcs:
                            print(f"      æ–°å¢å‡½æ•°: {', '.join(new_funcs)}")
                        if deleted_funcs:
                            print(f"      åˆ é™¤å‡½æ•°: {', '.join(deleted_funcs)}")
                        only_modified_existing_functions = False
                        break
                else:
                    print(f"    - âš ï¸ æ— æ³•è·å–æ–‡ä»¶å†…å®¹ï¼Œå¯èƒ½æ˜¯ç©ºæ–‡ä»¶æˆ–ä¸å­˜åœ¨: {file_path}")
                    only_modified_existing_functions = False
    
    # ç”Ÿæˆè¯¦ç»†æè¿°
    if only_modified_existing_functions and test_files and non_test_files:
        detailed_description = generate_detailed_description_with_llm(
            feature_description, pr_info, file_changes
        )
    
    if not detailed_description:
        if not test_files:
            print(f"  > â­ï¸ PR#{pr_number} ä¸åŒ…å«æµ‹è¯•æ–‡ä»¶å˜æ›´ï¼Œè·³è¿‡åˆ†æ")
        elif not non_test_files:
            print(f"  > â­ï¸ PR#{pr_number} ä¸åŒ…å«éæµ‹è¯•æ–‡ä»¶å˜æ›´ï¼Œè·³è¿‡åˆ†æ")
        elif not only_modified_existing_functions:
            print(f"  > â­ï¸ PR#{pr_number} åŒ…å«å‡½æ•°æ–°å¢æˆ–åˆ é™¤ï¼Œè·³è¿‡åˆ†æ")
        return None
    
    analysis = PRAnalysis(
        pr_number=pr_number,
        repo_name=repo_name,
        title=pr_info.get('title', ''),
        description=pr_info.get('body', ''),
        state=pr_info.get('state', ''),
        merged=pr_info.get('merged', False),
        base_commit=base_commit,
        head_commit=head_commit,
        file_changes=file_changes,
        detailed_description=detailed_description,
        has_tests=len(test_files) > 0,
        test_files=test_files,
        only_modified_existing_functions=only_modified_existing_functions,
        non_test_files=non_test_files,
        analyzed_at=time.strftime('%Y-%m-%d %H:%M:%S')
    )

    print(f"    - PR#{pr_number}: æœ‰æµ‹è¯•={analysis.has_tests}, æµ‹è¯•æ–‡ä»¶æ•°={len(analysis.test_files)}, ä»…ä¿®æ”¹å‡½æ•°={analysis.only_modified_existing_functions}, æ˜¯å¦ä»…ä¿®æ”¹å‡½æ•°={analysis.only_modified_existing_functions}")

    # ä¿å­˜åˆ°ç¼“å­˜
    if use_cache:
        save_pr_analysis_to_cache(analysis)
    
    return analysis

def enhance_feature_with_pr_analysis(feature, repo_name: str) -> Optional[EnhancedFeature]:
    """å¢å¼º feature å¯¹è±¡ï¼Œæ·»åŠ  PR è¯¦ç»†åˆ†æ"""
    pr_analyses = []
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºPRåˆ†æè¿›åº¦
    with tqdm(feature.pr_links, desc=f"åˆ†æPR", unit="pr", leave=False) as pbar:
        for pr_link in pbar:
            pr_number = extract_pr_number_from_url(pr_link)
            pbar.set_description(f"PR#{pr_number}")
            
            pr_analysis = analyze_pr(repo_name, pr_link, feature.description)
            if pr_analysis:
                pr_analyses.append(pr_analysis)
            
            # é¿å…APIé€Ÿç‡é™åˆ¶
            time.sleep(0.5)
    
    # å¦‚æœåªæœ‰ä¸€ä¸ªPRï¼Œç›´æ¥ä½¿ç”¨PRçš„è¯¦ç»†æè¿°
    if len(pr_analyses) == 1:
        feature_detailed_description = pr_analyses[0].detailed_description
    elif len(pr_analyses) > 1:
        # å¤šä¸ªPRæ—¶ï¼ŒåŸºäºæ‰€æœ‰PRåˆ†æç”Ÿæˆfeatureçš„è¯¦ç»†æè¿°
        feature_detailed_description = generate_feature_detailed_description(
            feature.description,
            feature.feature_type,
            pr_analyses
        )
    else:
        return None
    
    if feature_detailed_description:
        return EnhancedFeature(
            feature_type=feature.feature_type,
            description=feature.description,
            pr_analyses=pr_analyses,
            feature_detailed_description=feature_detailed_description
        )
    else:
        return None

def enhance_release_analysis_with_pr_details(release_analysis) -> List[EnhancedFeature]:
    """å¢å¼º release åˆ†æï¼Œåªå¤„ç† new_features æ·»åŠ  PR è¯¦ç»†ä¿¡æ¯"""
    print(f"--- å¼€å§‹åˆ†æ {release_analysis.tag_name} çš„æ–°åŠŸèƒ½ ---")
    
    enhanced_features = []
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºåŠŸèƒ½åˆ†æè¿›åº¦
    with tqdm(release_analysis.new_features, desc=f"åˆ†æåŠŸèƒ½", unit="feature", leave=False) as pbar:
        for feature in pbar:
            if feature.pr_links:
                pbar.set_description(f"åŠŸèƒ½: {feature.description[:30]}...")
                enhanced = enhance_feature_with_pr_analysis(feature, release_analysis.repo_name)
                if enhanced:
                    enhanced_features.append(enhanced)
                    pbar.write(f"    âœ… å·²åˆ†æåŠŸèƒ½: {feature.description[:50]}...")
                else:
                    pbar.write(f"    âš ï¸ è·³è¿‡åŠŸèƒ½: {feature.description[:50]}...")
    
    return enhanced_features