import json
import time
import toml
from typing import List, Dict, Optional
from dataclasses import dataclass, asdict
import openai
from pathlib import Path
from tqdm import tqdm

# --- é…ç½®åŠ è½½ ---
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_file = Path(__file__).parent / "config.toml"
    with open(config_file, 'r', encoding='utf-8') as f:
        return toml.load(f)

CONFIG = load_config()

# --- é…ç½®åŒº ---
OPENAI_API_KEY = CONFIG['common']['openai_api_key']
OPENAI_MODEL = CONFIG['common']['openai_model']

# ç¼“å­˜æ–‡ä»¶
ANALYSIS_CACHE_FILE = Path(__file__).parent / CONFIG['common']['output_dir'] / CONFIG['release_analyzer']['analysis_cache_file']

# --- æ•°æ®ç±»å®šä¹‰ ---

@dataclass
class FeatureAnalysis:
    """è¡¨ç¤ºä¸€ä¸ªåŠŸèƒ½åˆ†æç»“æœ"""
    feature_type: str  # 'new_feature', 'improvement', 'bug_fix', 'other'
    description: str
    pr_links: List[str]  # ç›¸å…³çš„PRé“¾æ¥
    
    def to_dict(self) -> Dict:
        return asdict(self)
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'FeatureAnalysis':
        return cls(**data)

@dataclass
class ReleaseAnalysis:
    """è¡¨ç¤ºä¸€ä¸ª release çš„åˆ†æç»“æœ"""
    tag_name: str
    repo_name: str
    new_features: List[FeatureAnalysis]
    improvements: List[FeatureAnalysis]
    bug_fixes: List[FeatureAnalysis]
    other_changes: List[FeatureAnalysis]
    processed_body: str  # å¤„ç†è¿‡PRé“¾æ¥çš„body
    analyzed_at: str
    
    def to_dict(self) -> Dict:
        return {
            'tag_name': self.tag_name,
            'repo_name': self.repo_name,
            'new_features': [f.to_dict() for f in self.new_features],
            'improvements': [f.to_dict() for f in self.improvements],
            'bug_fixes': [f.to_dict() for f in self.bug_fixes],
            'other_changes': [f.to_dict() for f in self.other_changes],
            'processed_body': self.processed_body,
            'analyzed_at': self.analyzed_at
        }
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'ReleaseAnalysis':
        return cls(
            tag_name=data['tag_name'],
            repo_name=data['repo_name'],
            new_features=[FeatureAnalysis.from_dict(f) for f in data.get('new_features', [])],
            improvements=[FeatureAnalysis.from_dict(f) for f in data.get('improvements', [])],
            bug_fixes=[FeatureAnalysis.from_dict(f) for f in data.get('bug_fixes', [])],
            other_changes=[FeatureAnalysis.from_dict(f) for f in data.get('other_changes', [])],
            processed_body=data.get('processed_body', ''),
            analyzed_at=data.get('analyzed_at', '')
        )

# --- ç¼“å­˜ç®¡ç† ---

def load_analysis_cache() -> Dict[str, ReleaseAnalysis]:
    """åŠ è½½åˆ†æç¼“å­˜"""
    if ANALYSIS_CACHE_FILE.exists():
        try:
            with open(ANALYSIS_CACHE_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                cache = {}
                for key, analysis_data in data.items():
                    cache[key] = ReleaseAnalysis.from_dict(analysis_data)
                print(f"âœ… ä»ç¼“å­˜åŠ è½½äº† {len(cache)} ä¸ªreleaseåˆ†æç»“æœ")
                return cache
        except Exception as e:
            print(f"âš ï¸ åŠ è½½åˆ†æç¼“å­˜å¤±è´¥: {e}")
            return {}
    return {}

def save_analysis_to_cache(analysis: ReleaseAnalysis):
    """ä¿å­˜åˆ†æç»“æœåˆ°ç¼“å­˜"""
    cache = {}
    if ANALYSIS_CACHE_FILE.exists():
        try:
            with open(ANALYSIS_CACHE_FILE, 'r', encoding='utf-8') as f:
                cache = json.load(f)
        except:
            pass
    
    cache_key = f"{analysis.repo_name}#{analysis.tag_name}"
    cache[cache_key] = analysis.to_dict()
    
    try:
        with open(ANALYSIS_CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(cache, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ å·²ä¿å­˜ {cache_key} çš„åˆ†æç»“æœåˆ°ç¼“å­˜")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜åˆ†æç¼“å­˜å¤±è´¥: {e}")

# --- LLM åˆ†æ ---

def analyze_release_with_llm(release_body: str, tag_name: str, repo_readme: str = "") -> Dict[str, List[Dict]]:
    """ä½¿ç”¨ LLM åˆ†æ release body ä¸­çš„åŠŸèƒ½å˜æ›´å’ŒPRé“¾æ¥"""
    
    client = openai.OpenAI(api_key=OPENAI_API_KEY, base_url="https://api.deepseek.com")
    
    # æ„å»ºåŒ…å«READMEä¸Šä¸‹æ–‡çš„prompt
    readme_context = ""
    if repo_readme.strip():
        # ä»é…ç½®æ–‡ä»¶è¯»å–å‚æ•°
        max_readme_length = CONFIG['release_analyzer']['max_readme_length']
        truncation_suffix = CONFIG['release_analyzer']['readme_truncation_suffix']
        
        # æˆªå–READMEï¼Œé¿å…promptè¿‡é•¿
        readme_excerpt = repo_readme[:max_readme_length]
        if len(repo_readme) > max_readme_length:
            readme_excerpt += truncation_suffix
        readme_context = f"""
Repository Context (README):
{readme_excerpt}

---
"""
    
    prompt = f"""
{readme_context}Analyze the following software release notes and categorize the changes into: new_features, improvements, bug_fixes, and other_changes.
For each change, extract any PR references (like #123, PR456, pull #789, etc.) mentioned in the text.

Release version: {tag_name}
Release notes:
{release_body}

Guidelines:
1. new_features: Brand new functionality, commands, rules, or capabilities
2. improvements: Enhancements to existing features, optimizations, performance improvements
3. bug_fixes: Bug fixes, error handling, crash fixes
4. other_changes: Documentation updates, dependency updates, refactoring (only if significant)
5. Extract PR numbers from various formats: #123, PR #456, pull 789, (#101), etc.
6. Only include PR numbers that are explicitly mentioned with the change
7. Ignore trivial changes like version bumps unless they're part of larger features
8. Use the repository context to better understand the project's domain and categorize changes more accurately

Return the result in JSON format:
{{
    "new_features": [
        {{
            "description": "Brief description of the new feature",
            "pr_ids": ["123", "456"]
        }}
    ],
    "improvements": [
        {{
            "description": "Brief description of the improvement", 
            "pr_ids": ["789"]
        }}
    ],
    "bug_fixes": [
        {{
            "description": "Brief description of the bug fix",
            "pr_ids": ["101"]
        }}
    ],
    "other_changes": [
        {{
            "description": "Brief description of other changes",
            "pr_ids": []
        }}
    ]
}}
"""

    try:
        response = client.chat.completions.create(
            model=OPENAI_MODEL,
            messages=[
                {"role": "system", "content": "You are a software development expert who specializes in analyzing release notes and categorizing software changes."},
                {"role": "user", "content": prompt}
            ],
            response_format={"type": "json_object"},
            temperature=0
        )
        
        content = response.choices[0].message.content
        if content is None:
            print("âš ï¸ LLMè¿”å›çš„å†…å®¹ä¸ºç©º")
            return {"new_features": [], "improvements": [], "bug_fixes": [], "other_changes": []}
        result = json.loads(content)
        return result
            
    except Exception as e:
        print(f"âš ï¸ LLMåˆ†æå¤±è´¥: {e}")
        return {"new_features": [], "improvements": [], "bug_fixes": [], "other_changes": []}

# --- ä¸»è¦åŠŸèƒ½å‡½æ•° ---

def analyze_release(release, repo_name: str, repo_readme: str = "", use_cache: bool = True) -> Optional[ReleaseAnalysis]:
    """åˆ†æå•ä¸ª release"""
    cache_key = f"{repo_name}#{release.tag_name}"
    
    # æ£€æŸ¥ç¼“å­˜
    if use_cache:
        cache = load_analysis_cache()
        if cache_key in cache:
            print(f"  > ğŸ”„ ä»ç¼“å­˜åŠ è½½ {release.tag_name} çš„åˆ†æç»“æœ")
            return cache[cache_key]
    
    print(f"  > ğŸ” æ­£åœ¨åˆ†æ {release.tag_name} çš„release...")
    
    # LLM åˆ†æï¼Œä¼ å…¥READMEå†…å®¹
    llm_result = analyze_release_with_llm(release.body, release.tag_name, repo_readme)
    
    # è½¬æ¢ä¸º FeatureAnalysis å¯¹è±¡
    def convert_to_feature_analysis(items: List[Dict], feature_type: str) -> List[FeatureAnalysis]:
        features = []
        for item in items:
            pr_links = []
            # ä»LLMç»“æœä¸­è·å–PR IDå¹¶è½¬æ¢ä¸ºå®Œæ•´é“¾æ¥
            if 'pr_ids' in item:
                for pr_id in item['pr_ids']:
                    pr_links.append(f"https://github.com/{repo_name}/pull/{pr_id}")
            
            features.append(FeatureAnalysis(
                feature_type=feature_type,
                description=item.get('description', ''),
                pr_links=pr_links
            ))
        return features
    
    analysis = ReleaseAnalysis(
        tag_name=release.tag_name,
        repo_name=repo_name,
        new_features=convert_to_feature_analysis(llm_result.get('new_features', []), 'new_feature'),
        improvements=convert_to_feature_analysis(llm_result.get('improvements', []), 'improvement'),
        bug_fixes=convert_to_feature_analysis(llm_result.get('bug_fixes', []), 'bug_fix'),
        other_changes=convert_to_feature_analysis(llm_result.get('other_changes', []), 'other'),
        processed_body=release.body,
        analyzed_at=time.strftime('%Y-%m-%d %H:%M:%S')
    )
    
    # ä¿å­˜åˆ°ç¼“å­˜
    if use_cache:
        save_analysis_to_cache(analysis)
    
    return analysis

def analyze_repository_releases(repository) -> List[ReleaseAnalysis]:
    """åˆ†æä»“åº“çš„æ‰€æœ‰ major releases"""
    print(f"--- å¼€å§‹åˆ†æä»“åº“ {repository.full_name} çš„releaseåŠŸèƒ½ ---")
    
    analyses = []
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºåˆ†æè¿›åº¦
    with tqdm(repository.major_releases, desc=f"åˆ†æ{repository.full_name}", unit="release") as pbar:
        for release in pbar:
            pbar.set_description(f"åˆ†æ: {release.tag_name}")
            
            # ä¼ å…¥READMEå†…å®¹åˆ°åˆ†æå‡½æ•°
            analysis = analyze_release(release, repository.full_name, repository.readme_content)
            if analysis:
                analyses.append(analysis)
                # æ˜¾ç¤ºåˆ†æç»“æœæ‘˜è¦
                new_features_count = len(analysis.new_features)
                improvements_count = len(analysis.improvements)
                bug_fixes_count = len(analysis.bug_fixes)
                pbar.write(f"    âœ… {release.tag_name}: æ–°åŠŸèƒ½({new_features_count}) æ”¹è¿›({improvements_count}) ä¿®å¤({bug_fixes_count})")
            
            # é¿å…APIé€Ÿç‡é™åˆ¶
            time.sleep(1)
    
    return analyses