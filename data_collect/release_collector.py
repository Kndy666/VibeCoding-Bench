import requests
import time
import re
import json
import toml
from pathlib import Path
from dataclasses import dataclass, asdict
from typing import List, Dict, Tuple
from datetime import datetime
from tqdm import tqdm

# --- é…ç½®åŠ è½½ ---
def load_config():
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    config_file = Path(__file__).parent / "config.toml"
    with open(config_file, 'r', encoding='utf-8') as f:
        return toml.load(f)

CONFIG = load_config()

# --- é…ç½®åŒº ---

TOKEN = CONFIG['common']['github_token']
HEADERS = {'Authorization': f'Bearer {TOKEN}', 'Accept': 'application/vnd.github.v3+json'}

# çˆ¬å–æ¨¡å¼é…ç½®
CRAWL_MODE = CONFIG['common']['crawl_mode']
CRAWL_JSON_FILE = Path(__file__).parent / CONFIG['common']['crawl_json_file']

# ç­›é€‰é˜ˆå€¼
MIN_STARS = CONFIG['release_collector']['min_stars_range']
RANK_START = CONFIG['release_collector']['rank_start']
RANK_END = CONFIG['release_collector']['rank_end']
MIN_RELEASES = CONFIG['release_collector']['min_releases']
MIN_RELEASE_BODY_LENGTH = CONFIG['release_collector']['min_release_body_length']
MIN_RELEASE_DATE = CONFIG['release_collector']['min_release_date']
EXCLUDED_TOPICS = set(CONFIG['release_collector']['excluded_topics'])

# Test caseç›¸å…³é…ç½®
TEST_DIRECTORIES = CONFIG['release_collector']['test_directories']
TEST_FILE_PATTERNS = CONFIG['release_collector']['test_file_patterns']
BOT_USERS = set(CONFIG['release_collector']['bot_users'])

# ç¼“å­˜æ–‡ä»¶è·¯å¾„
CACHE_FILE = Path(__file__).parent / CONFIG['common']['output_dir'] / CONFIG['release_collector']['cache_file']

# --- æ•°æ®ç±»å®šä¹‰ ---

@dataclass
class Release:
    """è¡¨ç¤ºä¸€ä¸ªå‘å¸ƒç‰ˆæœ¬"""
    tag_name: str
    name: str
    body: str
    published_at: str
    target_commitish: str
    version_tuple: Tuple[int, ...]
    version_key: str
    
    def to_dict(self) -> Dict:
        data = asdict(self)
        return data
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'Release':
        release = cls(**data)
        return release

@dataclass
class Repository:
    """è¡¨ç¤ºä¸€ä¸ªä»“åº“åŠå…¶å‘å¸ƒä¿¡æ¯"""
    full_name: str
    stargazers_count: int
    size: int
    topics: List[str]
    releases_count: int
    major_releases: List[Release]
    readme_content: str
    ci_configs: Dict[str, str]  # æ–°å¢ï¼šCI/CDé…ç½®æ–‡ä»¶å†…å®¹
    processed_at: str
    
    def to_dict(self) -> Dict:
        data = asdict(self)
        data['major_releases'] = [release.to_dict() for release in self.major_releases]
        return data
    
    @classmethod
    def from_dict(cls, data: Dict) -> 'Repository':
        repo = cls(**data)
        repo.major_releases = [Release.from_dict(release_data) for release_data in data.get("major_releases", [])]
        return repo

# --- ç¼“å­˜ç®¡ç†å‡½æ•° ---

def load_processed_repos() -> Dict[str, Repository]:
    """ä»JSONæ–‡ä»¶åŠ è½½å·²å¤„ç†çš„ä»“åº“ä¿¡æ¯ã€‚"""
    if CACHE_FILE.exists():
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                data = json.load(f)
                processed_repos = {}
                for repo_name, repo_data in data.items():
                    processed_repos[repo_name] = Repository.from_dict(repo_data)
                print(f"âœ… ä»ç¼“å­˜æ–‡ä»¶åŠ è½½äº† {len(processed_repos)} ä¸ªå·²å¤„ç†çš„ä»“åº“")
                return processed_repos
        except (json.JSONDecodeError, Exception) as e:
            print(f"âš ï¸ åŠ è½½ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}ï¼Œå°†é‡æ–°å¼€å§‹å¤„ç†")
            return {}
    else:
        print("ğŸ“ ç¼“å­˜æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°†åˆ›å»ºæ–°çš„ç¼“å­˜")
        return {}

def save_processed_repo(repository: Repository):
    """ä¿å­˜å•ä¸ªä»“åº“çš„å¤„ç†ç»“æœåˆ°JSONæ–‡ä»¶ã€‚"""
    # åŠ è½½ç°æœ‰æ•°æ®
    processed_repos_dict = {}
    if CACHE_FILE.exists():
        try:
            with open(CACHE_FILE, 'r', encoding='utf-8') as f:
                processed_repos_dict = json.load(f)
        except:
            pass
    
    # æ·»åŠ æ–°çš„ä»“åº“æ•°æ®
    processed_repos_dict[repository.full_name] = repository.to_dict()
    
    # ä¿å­˜åˆ°æ–‡ä»¶
    try:
        with open(CACHE_FILE, 'w', encoding='utf-8') as f:
            json.dump(processed_repos_dict, f, indent=2, ensure_ascii=False)
        print(f"ğŸ’¾ å·²ä¿å­˜ä»“åº“ {repository.full_name} çš„å¤„ç†ç»“æœåˆ°ç¼“å­˜")
    except Exception as e:
        print(f"âš ï¸ ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")

# --- æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def get_candidate_repos():
    """ä» GitHub API è·å–æŒ‡å®šæ’åèŒƒå›´å†…çš„ Python ä»“åº“ä½œä¸ºå€™é€‰æ± ã€‚"""
    print(f"è·å– Stars >= {MIN_STARS} çš„ Python ä»“åº“ï¼Œç­›é€‰æ’å {RANK_START}-{RANK_END}...")
    
    API_URL = "https://api.github.com/search/repositories"
    PARAMS = {
        'q': f'language:python stars:>={MIN_STARS}',
        'sort': 'stars',
        'order': 'desc',
        'per_page': 100  # GitHub API å•æ¬¡è¯·æ±‚æœ€å¤š100ä¸ª
    }
    
    all_repos = []
    page = 1
    current_repo_count = 0
    
    try:
        while True:
            params_with_page = PARAMS.copy()
            params_with_page['page'] = page
            
            response = requests.get(API_URL, params=params_with_page, headers=HEADERS)
            response.raise_for_status()
            
            data = response.json()
            repos = data.get('items', [])
            
            if not repos:  # æ²¡æœ‰æ›´å¤šç»“æœ
                break
                
            # æ£€æŸ¥å½“å‰é¡µçš„ä»“åº“æ’åèŒƒå›´
            page_start_rank = current_repo_count + 1
            page_end_rank = current_repo_count + len(repos)
            
            print(f"âœ… å·²è·å–ç¬¬ {page} é¡µï¼Œä»“åº“æ’å {page_start_rank}-{page_end_rank}")
            
            # å¦‚æœå½“å‰é¡µçš„èµ·å§‹æ’åå·²ç»è¶…è¿‡äº†æˆ‘ä»¬éœ€è¦çš„ç»“æŸæ’åï¼Œåœæ­¢è·å–
            if page_start_rank > RANK_END:
                print(f"å·²è¶…è¿‡ç›®æ ‡æ’åèŒƒå›´ {RANK_END}ï¼Œåœæ­¢è·å–")
                break
            
            # ç­›é€‰å‡ºåœ¨ç›®æ ‡æ’åèŒƒå›´å†…çš„ä»“åº“
            for i, repo in enumerate(repos):
                repo_rank = current_repo_count + i + 1
                if RANK_START <= repo_rank <= RANK_END:
                    repo['rank'] = repo_rank  # æ·»åŠ æ’åä¿¡æ¯
                    all_repos.append(repo)
            
            current_repo_count += len(repos)
            
            # å¦‚æœå·²ç»è·å–åˆ°ç›®æ ‡æ’åèŒƒå›´çš„æ‰€æœ‰ä»“åº“ï¼Œåœæ­¢è·å–
            if page_end_rank >= RANK_END:
                print(f"å·²è·å–åˆ°ç›®æ ‡æ’åèŒƒå›´ {RANK_END}ï¼Œåœæ­¢è·å–")
                break
                
            # GitHubæœç´¢APIæœ€å¤šè¿”å›1000ä¸ªç»“æœï¼Œä¸”æœ‰åˆ†é¡µé™åˆ¶
            if current_repo_count >= data.get('total_count', 0) or page >= 10:
                break
                
            page += 1
            time.sleep(0.5)  # é¿å…APIé™åˆ¶
            
        print(f"âœ… æ€»å…±è·å–åˆ° {len(all_repos)} ä¸ªæ’ååœ¨ {RANK_START}-{RANK_END} èŒƒå›´å†…çš„ä»“åº“")
        return all_repos
        
    except requests.exceptions.HTTPError as e:
        print(f"HTTP é”™è¯¯: {e}")
        if e.response.status_code == 403:
            print("API é€Ÿç‡é™åˆ¶å·²è¶…å‡ºã€‚è¯·ä½¿ç”¨ Token æˆ–ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•ã€‚")
        return []

def has_test_cases(repo_full_name: str) -> bool:
    """æ£€æŸ¥ä»“åº“æ˜¯å¦åŒ…å«æµ‹è¯•ç”¨ä¾‹"""
    print(f"  > æ­£åœ¨æ£€æŸ¥ {repo_full_name} æ˜¯å¦æœ‰test case...")
    
    try:
        # 1. æ£€æŸ¥æ˜¯å¦æœ‰æµ‹è¯•ç›®å½•
        contents_url = f"https://api.github.com/repos/{repo_full_name}/contents"
        time.sleep(0.5)
        response = requests.get(contents_url, headers=HEADERS)
        response.raise_for_status()
        
        contents = response.json()
        
        # æ£€æŸ¥æ ¹ç›®å½•ä¸‹æ˜¯å¦æœ‰æµ‹è¯•ç›¸å…³ç›®å½•
        has_test_directory = False
        test_directories_found = []
        for item in contents:
            if item.get('type') == 'dir':
                dir_name = item.get('name', '').lower()
                if any(test_dir in dir_name for test_dir in TEST_DIRECTORIES):
                    print(f"  > âœ… å‘ç°æµ‹è¯•ç›®å½•: {item.get('name')}")
                    has_test_directory = True
                    test_directories_found.append(item.get('name'))
        
        # 2. æ£€æŸ¥æ ¹ç›®å½•ä¸‹æ˜¯å¦æœ‰æµ‹è¯•æ–‡ä»¶
        for item in contents:
            if item.get('type') == 'file':
                file_name = item.get('name', '')
                if any(re.match(pattern, file_name) for pattern in TEST_FILE_PATTERNS):
                    print(f"  > âœ… å‘ç°æµ‹è¯•æ–‡ä»¶: {file_name}")
                    return True
        
        # 3. åªæœ‰åœ¨æ ¹ç›®å½•ä¸‹å‘ç°æµ‹è¯•ç›®å½•æ—¶ï¼Œæ‰é€’å½’æ£€æŸ¥æµ‹è¯•ç›®å½•çš„å†…å®¹
        if has_test_directory:
            def check_directory_for_tests(repo_name, directory_path):
                """é€’å½’æ£€æŸ¥ç›®å½•ä¸­æ˜¯å¦åŒ…å«Pythonæµ‹è¯•æ–‡ä»¶"""
                try:
                    dir_url = f"https://api.github.com/repos/{repo_name}/contents/{directory_path}"
                    time.sleep(0.5)
                    response = requests.get(dir_url, headers=HEADERS)
                    if response.status_code == 200:
                        contents = response.json()
                        if isinstance(contents, list):
                            # ä¸€æ¬¡æ€§æ£€æŸ¥è¯¥ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶
                            files = [item for item in contents if item.get('type') == 'file']
                            for item in files:
                                file_name = item.get('name', '')
                                # æ£€æŸ¥æ˜¯å¦æ˜¯Pythonæ–‡ä»¶æˆ–æµ‹è¯•æ–‡ä»¶
                                if file_name.endswith('.py') or any(re.match(pattern, file_name) for pattern in TEST_FILE_PATTERNS):
                                    print(f"  > âœ… å‘ç°æµ‹è¯•ç›®å½• {directory_path} ä¸­çš„Pythonæ–‡ä»¶: {file_name}")
                                    return True
                            
                            # ç„¶åé€’å½’æ£€æŸ¥å­ç›®å½•
                            directories = [item for item in contents if item.get('type') == 'dir']
                            for dir_item in directories:
                                sub_dir_path = f"{directory_path}/{dir_item.get('name')}"
                                if check_directory_for_tests(repo_name, sub_dir_path):
                                    return True
                    return False
                except Exception as e:
                    print(f"  > âš ï¸ æ£€æŸ¥ç›®å½• {directory_path} æ—¶å‡ºé”™: {e}")
                    return False
            
            # å¯¹å‘ç°çš„æ¯ä¸ªæµ‹è¯•ç›®å½•è¿›è¡Œé€’å½’æ£€æŸ¥
            for test_dir in test_directories_found:
                if check_directory_for_tests(repo_full_name, test_dir):
                    return True

        print(f"  > âŒ æœªå‘ç°æ˜æ˜¾çš„æµ‹è¯•ç”¨ä¾‹")
        return False
        
    except requests.exceptions.HTTPError as e:
        print(f"  > âš ï¸ æ£€æŸ¥æµ‹è¯•ç”¨ä¾‹æ—¶å‡ºé”™: {e}")
        return False
    except Exception as e:
        print(f"  > âš ï¸ æ£€æŸ¥æµ‹è¯•ç”¨ä¾‹æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        return False

def is_valid_release(release_data: dict) -> bool:
    """æ£€æŸ¥releaseæ˜¯å¦æœ‰æ•ˆï¼ˆébotç”Ÿæˆä¸”å†…å®¹å……å®ä¸”åœ¨æŒ‡å®šæ—¥æœŸä¹‹åï¼‰"""
    # æ£€æŸ¥æ˜¯å¦ç”±botç”Ÿæˆ
    author_login = release_data.get('author', {}).get('login', '')
    if author_login in BOT_USERS:
        return False
    
    # æ£€æŸ¥release bodyé•¿åº¦
    body = release_data.get('body', '') or ''  # ç¡®ä¿bodyä¸ä¸ºNone
    if len(body.strip()) < MIN_RELEASE_BODY_LENGTH:
        return False
    
    # æ£€æŸ¥å‘å¸ƒæ—¶é—´æ˜¯å¦åœ¨æŒ‡å®šæ—¥æœŸä¹‹å
    published_at = release_data.get('published_at', '')
    if published_at:
        try:
            release_date = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
            min_date = datetime.fromisoformat(MIN_RELEASE_DATE + 'T00:00:00+00:00')
            if release_date < min_date:
                return False
        except Exception:
            # å¦‚æœæ—¥æœŸè§£æå¤±è´¥ï¼Œè·³è¿‡æ­¤release
            return False
    else:
        # å¦‚æœæ²¡æœ‰å‘å¸ƒæ—¥æœŸï¼Œè·³è¿‡æ­¤release
        return False
    
    return True

def filter_by_metadata_and_releases(repos):
    """é€šè¿‡ API å¯¹ä»“åº“è¿›è¡Œå®è§‚æŒ‡æ ‡å’Œreleaseæ•°é‡çš„åˆæ­¥ç­›é€‰ã€‚"""
    print(f"é€šè¿‡å®è§‚æŒ‡æ ‡å’Œreleaseæ•°é‡è¿›è¡Œåˆæ­¥ç­›é€‰ï¼ˆæ’åèŒƒå›´ {RANK_START}-{RANK_END}ï¼‰...")
    
    filtered_repos = []
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºç­›é€‰è¿›åº¦
    with tqdm(repos, desc="ç­›é€‰ä»“åº“", unit="repo") as pbar:
        for repo in pbar:
            repo_name = repo['full_name']
            repo_rank = repo.get('rank', 0)
            pbar.set_description(f"æ£€æŸ¥: {repo_name} (æ’å#{repo_rank})")

            # 1. æ£€æŸ¥å®è§‚æŒ‡æ ‡ï¼ˆtopicsç­›é€‰ï¼‰
            repo_topics = set(repo.get('topics', []))
            if repo_topics.intersection(EXCLUDED_TOPICS):
                pbar.write(f"  âŒ {repo_name} (#{repo_rank}): åŒ…å«æ’é™¤çš„ä¸»é¢˜")
                continue

            # 2. æ£€æŸ¥æ˜¯å¦æœ‰test case
            if not has_test_cases(repo_name):
                pbar.write(f"  âŒ {repo_name} (#{repo_rank}): æ— æµ‹è¯•ç”¨ä¾‹")
                continue

            # 3. æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆrelease
            releases_url = f"https://api.github.com/repos/{repo_name}/releases"
            try:
                # æ¯æ¬¡æ£€æŸ¥ä¹‹é—´ç¨ä½œåœé¡¿ï¼Œé¿å… API è¶…é™
                time.sleep(1)
                response = requests.get(releases_url, headers=HEADERS)
                response.raise_for_status()
                
                releases = response.json()
                # è¿‡æ»¤æœ‰æ•ˆçš„release
                valid_releases = [r for r in releases if is_valid_release(r)]
                
                if len(valid_releases) >= MIN_RELEASES:
                    repo['releases_count'] = len(valid_releases)
                    repo['releases_data'] = valid_releases  # ä¿å­˜è¿‡æ»¤åçš„releasesæ•°æ®
                    filtered_repos.append(repo)
                    pbar.write(f"  âœ… {repo_name} (#{repo_rank}): é€šè¿‡åˆç­›! Stars: {repo['stargazers_count']}, æœ‰æ•ˆrelease: {len(valid_releases)}")
                else:
                    pbar.write(f"  âŒ {repo_name} (#{repo_rank}): æœ‰æ•ˆreleaseæ•°é‡ä¸è¶³ï¼Œåªæœ‰ {len(valid_releases)} ä¸ª")

            except requests.exceptions.HTTPError as e:
                if e.response.status_code == 403:
                    pbar.write("\nAPI é€Ÿç‡é™åˆ¶è¶…å‡ºï¼Œåˆç­›æå‰ç»ˆæ­¢ã€‚")
                    break
                pbar.write(f"  âŒ {repo_name} (#{repo_rank}): API é”™è¯¯: {e.response.status_code}")
    
    return filtered_repos

def extract_version_components(tag_name):
    """
    ä»æ ‡ç­¾åæå–ç‰ˆæœ¬å·ç»„ä»¶ã€‚
    æ”¯æŒå¤šç§æ ¼å¼å¦‚ï¼šv1.2.3, 1.2.3, 1-2-3, release-1.2.3, version.1.2.3ç­‰
    å¤„ç†ç‰ˆæœ¬å·ä¸­å¯èƒ½å­˜åœ¨çš„ç©ºæ ¼ï¼Œå¦‚"v 1.2.3"æˆ–"1. 2. 3"
    
    è¿”å›ï¼š
    - å¦‚æœæˆåŠŸæå–åˆ°ç‰ˆæœ¬å·ï¼Œè¿”å›ä¸€ä¸ªç‰ˆæœ¬å·å…ƒç»„ (major, minor, patch, ...)
    - å¦‚æœæ— æ³•æå–ï¼Œè¿”å› None
    """
    # å…ˆæ¸…ç†è¾“å…¥å­—ç¬¦ä¸²ï¼Œå»é™¤é¦–å°¾ç©ºæ ¼
    tag_name = tag_name.strip()
    
    # 1. ä¼˜å…ˆå°è¯•ç›´æ¥åŒ¹é…ç‰ˆæœ¬å·æ¨¡å¼ï¼ˆä¸ä¾èµ–å‰ç¼€åˆ¤æ–­ï¼‰
    # åŒ¹é…æ ¼å¼ï¼šæ•°å­—.æ•°å­—.æ•°å­—.æ•°å­—... æˆ– æ•°å­—-æ•°å­—-æ•°å­—... æˆ– æ•°å­—_æ•°å­—_æ•°å­—...
    direct_version_pattern = re.compile(r'(\d+)(?:\s*[.\-_]\s*(\d+))?(?:\s*[.\-_]\s*(\d+))?(?:\s*[.\-_]\s*(\d+))?')
    
    # é¦–å…ˆå°è¯•ä»å­—ç¬¦ä¸²å¼€å¤´åŒ¹é…ç‰ˆæœ¬å·
    match = direct_version_pattern.match(tag_name)
    if match:
        version_tuple = tuple(int(group) for group in match.groups() if group is not None)
        if version_tuple:  # ç¡®ä¿è‡³å°‘æœ‰ä¸€ä¸ªç‰ˆæœ¬å·ç»„ä»¶
            return version_tuple
    
    # 2. å¦‚æœå¼€å¤´æ²¡æœ‰åŒ¹é…åˆ°ï¼Œå°è¯•ç§»é™¤å¸¸è§å‰ç¼€åå†åŒ¹é…
    version_string = tag_name
    common_prefixes = ['version', 'release', 'ver', 'rel', 'v']  # æŒ‰é•¿åº¦æ’åºï¼Œå…ˆåŒ¹é…é•¿çš„
    
    for prefix in common_prefixes:
        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›´ç²¾ç¡®åœ°åŒ¹é…å‰ç¼€
        prefix_pattern = re.compile(rf'^{re.escape(prefix)}[.\-_\s]*', re.IGNORECASE)
        if prefix_pattern.match(tag_name):
            version_string = prefix_pattern.sub('', tag_name).strip()
            break
    
    # 3. åœ¨å¤„ç†åçš„å­—ç¬¦ä¸²ä¸­æŸ¥æ‰¾ç‰ˆæœ¬å·
    match = direct_version_pattern.match(version_string)
    if match:
        version_tuple = tuple(int(group) for group in match.groups() if group is not None)
        if version_tuple:
            return version_tuple
    
    # 4. æœ€åå°è¯•åœ¨æ•´ä¸ªå­—ç¬¦ä¸²ä¸­æŸ¥æ‰¾ä»»ä½•ä½ç½®çš„ç‰ˆæœ¬å·æ¨¡å¼
    match = direct_version_pattern.search(tag_name)
    if match:
        version_tuple = tuple(int(group) for group in match.groups() if group is not None)
        if version_tuple:
            return version_tuple
    
    return None

def get_major_releases(repo_full_name: str, releases_data, limit=5) -> List[Release]:
    """è·å–ä»“åº“çš„ä¸»è¦ç‰ˆæœ¬releaseï¼ˆæŒ‰ä¸»ç‰ˆæœ¬å·åˆ†ç»„åå–æ¯ç»„æœ€æ–°çš„ï¼‰ã€‚"""
    print(f"  > æ­£åœ¨è·å– {repo_full_name} çš„ä¸»è¦ç‰ˆæœ¬release...")
    
    all_releases = releases_data
    print(f"  > ä½¿ç”¨å·²è·å–çš„ {len(all_releases)} ä¸ªæœ‰æ•ˆreleasesæ•°æ®")
    
    valid_releases = []
    
    for release in all_releases:
        tag_name = release.get('tag_name', '')
        
        # ä½¿ç”¨æ–°çš„ç‰ˆæœ¬æå–å‡½æ•°
        version_tuple = extract_version_components(tag_name)
        
        if version_tuple:
            # è·³è¿‡é¢„å‘å¸ƒç‰ˆæœ¬ (åŒ…å« alpha, beta, rc, a, b ç­‰æ ‡è¯†)
            if re.search(r'(alpha|beta|rc|a\d+|b\d+)', tag_name.lower()):
                continue
            
            release_obj = Release(
                tag_name=tag_name,
                name=release.get('name', ''),
                body=release.get('body', ''),
                published_at=release.get('published_at', ''),
                target_commitish=release.get('target_commitish', ''),
                version_tuple=version_tuple,
                version_key='.'.join(str(v) for v in version_tuple),
            )
            valid_releases.append(release_obj)
    
    # æŒ‰ç‰ˆæœ¬å·æ’åºï¼Œå–æœ€æ–°çš„å‡ ä¸ªç‰ˆæœ¬
    valid_releases.sort(key=lambda x: x.version_tuple, reverse=True)
    result = valid_releases[:limit]  # åªå–å‰å‡ ä¸ªç‰ˆæœ¬
    
    print(f"  > æˆåŠŸè·å– {len(result)} ä¸ªä¸»è¦ç‰ˆæœ¬release")
    if result:
        version_list = ', '.join([r.version_key for r in result])
        print(f"  > é€‰æ‹©çš„ç‰ˆæœ¬: {version_list}")
    return result

def get_repository_readme(repo_full_name: str) -> str:
    """è·å–ä»“åº“çš„READMEå†…å®¹"""
    print(f"  > æ­£åœ¨è·å– {repo_full_name} çš„README...")
    
    try:
        # è·å–ä»“åº“æ ¹ç›®å½•çš„æ‰€æœ‰æ–‡ä»¶
        root_url = f"https://api.github.com/repos/{repo_full_name}/contents"
        time.sleep(0.5)  # é¿å…APIé™åˆ¶
        response = requests.get(root_url, headers=HEADERS)
        response.raise_for_status()
        
        contents = response.json()
        
        # å¸¸è§çš„READMEæ–‡ä»¶åæ¨¡å¼
        readme_patterns = [r'^readme\.md$', r'^readme\.rst$', r'^readme\.txt$', r'^readme$']
        
        # åœ¨æœ¬åœ°æ£€æŸ¥æ–‡ä»¶åˆ—è¡¨æ˜¯å¦åŒ…å«README
        for item in contents:
            if item.get('type') == 'file':
                file_name = item.get('name', '').lower()
                if any(re.match(pattern, file_name, re.IGNORECASE) for pattern in readme_patterns):
                    # æ‰¾åˆ°READMEæ–‡ä»¶ï¼Œè·å–å†…å®¹
                    download_url = item.get('download_url')
                    if download_url:
                        content_response = requests.get(download_url, headers=HEADERS)
                        content_response.raise_for_status()
                        readme_content = content_response.text
                        print(f"  > âœ… æˆåŠŸè·å–README ({item.get('name')}), é•¿åº¦: {len(readme_content)} å­—ç¬¦")
                        return readme_content
        
        print(f"  > âŒ æœªæ‰¾åˆ°READMEæ–‡ä»¶")
        return ""
    
    except Exception as e:
        print(f"  > âš ï¸ è·å–READMEæ—¶å‡ºé”™: {e}")
        return ""

def get_ci_configs(repo_full_name: str) -> Dict[str, str]:
    """è·å–ä»“åº“çš„CI/CDé…ç½®æ–‡ä»¶åˆ—è¡¨å’Œä¸‹è½½é“¾æ¥"""
    print(f"  > æ­£åœ¨è·å– {repo_full_name} çš„CI/CDé…ç½®æ–‡ä»¶åˆ—è¡¨...")
    
    ci_configs = {}
    
    try:
        # æ£€æŸ¥.github/workflowsç›®å½•æ˜¯å¦å­˜åœ¨
        workflows_url = f"https://api.github.com/repos/{repo_full_name}/contents/.github/workflows"
        time.sleep(0.5)  # é¿å…APIé™åˆ¶
        response = requests.get(workflows_url, headers=HEADERS)
        
        # å¦‚æœç›®å½•å­˜åœ¨ï¼Œæ”¶é›†å…¶ä¸­çš„æ‰€æœ‰YAMLæ–‡ä»¶ä¿¡æ¯
        if response.status_code == 200:
            contents = response.json()
            
            for item in contents:
                if item.get('type') == 'file' and (item.get('name', '').endswith('.yml') or item.get('name', '').endswith('.yaml')):
                    file_name = item.get('name', '')
                    file_path = f".github/workflows/{file_name}"
                    download_url = item.get('download_url', '')
                    
                    if download_url:
                        ci_configs[file_path] = download_url
                        print(f"  > âœ… å‘ç°CIé…ç½®: {file_path}")
        
        if ci_configs:
            print(f"  > âœ… å…±å‘ç° {len(ci_configs)} ä¸ªCIé…ç½®æ–‡ä»¶")
        else:
            print(f"  > âŒ æœªæ‰¾åˆ°CIé…ç½®æ–‡ä»¶")
        
        return ci_configs
    
    except Exception as e:
        print(f"  > âš ï¸ è·å–CIé…ç½®åˆ—è¡¨æ—¶å‡ºé”™: {e}")
        return {}

def process_single_repository(repo: Dict, use_cache: bool = True) -> Repository:
    """å¤„ç†å•ä¸ªä»“åº“ï¼Œè·å–å…¶è¯¦ç»†ä¿¡æ¯"""
    repo_name = repo['full_name']
    
    # è·å–ä¸»è¦ç‰ˆæœ¬releaseï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é™åˆ¶
    major_releases = get_major_releases(
        repo_name, 
        releases_data=repo.get('releases_data'), 
        limit=CONFIG['release_collector']['default_release_limit']
    )
    if not major_releases:
        raise ValueError(f"æ— æ³•è·å–ä¸»è¦ç‰ˆæœ¬release")
        
    # è·å–READMEå†…å®¹
    readme_content = get_repository_readme(repo_name)

    # è·å–CI/CDé…ç½®æ–‡ä»¶
    ci_configs = get_ci_configs(repo_name)
    
    # åˆ›å»ºRepositoryå¯¹è±¡
    repository = Repository(
        full_name=repo_name,
        stargazers_count=repo['stargazers_count'],
        size=repo['size'],
        topics=repo.get('topics', []),
        releases_count=repo['releases_count'],
        major_releases=major_releases,
        readme_content=readme_content,
        ci_configs=ci_configs,
        processed_at=time.strftime('%Y-%m-%d %H:%M:%S')
    )
    
    # ä¿å­˜åˆ°ç¼“å­˜
    if use_cache:
        save_processed_repo(repository)
    
    return repository

def get_specified_repos():
    """ä» crawl.json æ–‡ä»¶è·å–æŒ‡å®šçš„ä»“åº“åˆ—è¡¨"""
    print(f"ä»æŒ‡å®šæ–‡ä»¶è·å–ä»“åº“åˆ—è¡¨: {CRAWL_JSON_FILE}")
    
    if not CRAWL_JSON_FILE.exists():
        print(f"âŒ æŒ‡å®šçš„ä»“åº“æ–‡ä»¶ä¸å­˜åœ¨: {CRAWL_JSON_FILE}")
        return []
    
    try:
        with open(CRAWL_JSON_FILE, 'r', encoding='utf-8') as f:
            crawl_data = json.load(f)
        
        # æ”¶é›†æ‰€æœ‰ç±»åˆ«çš„ä»“åº“
        all_repos = []
        for category, repos in crawl_data.items():
            print(f"âœ… åŠ è½½ç±»åˆ« '{category}': {len(repos)} ä¸ªä»“åº“")
            for repo_name in repos:
                all_repos.append(repo_name)
        
        print(f"âœ… æ€»å…±åŠ è½½ {len(all_repos)} ä¸ªæŒ‡å®šä»“åº“")
        
        # ä¸ºæ¯ä¸ªä»“åº“è·å–è¯¦ç»†ä¿¡æ¯
        detailed_repos = []
        with tqdm(all_repos, desc="è·å–ä»“åº“ä¿¡æ¯", unit="repo") as pbar:
            for repo_name in pbar:
                pbar.set_description(f"è·å–: {repo_name}")
                try:
                    repo_info = get_repository_info(repo_name)
                    if repo_info:
                        detailed_repos.append(repo_info)
                        pbar.write(f"  âœ… {repo_name}: Stars {repo_info['stargazers_count']}")
                    else:
                        pbar.write(f"  âŒ {repo_name}: è·å–ä¿¡æ¯å¤±è´¥")
                except Exception as e:
                    pbar.write(f"  âŒ {repo_name}: {str(e)}")
                    continue
                
                time.sleep(0.5)  # é¿å…APIé™åˆ¶
        
        print(f"âœ… æˆåŠŸè·å– {len(detailed_repos)} ä¸ªä»“åº“çš„è¯¦ç»†ä¿¡æ¯")
        return detailed_repos
        
    except Exception as e:
        print(f"âŒ è¯»å–æŒ‡å®šä»“åº“æ–‡ä»¶å¤±è´¥: {e}")
        return []

def get_repository_info(repo_name: str) -> Dict:
    """è·å–å•ä¸ªä»“åº“çš„è¯¦ç»†ä¿¡æ¯"""
    try:
        repo_url = f"https://api.github.com/repos/{repo_name}"
        response = requests.get(repo_url, headers=HEADERS)
        response.raise_for_status()
        
        repo_data = response.json()
        
        # è¿”å›ä¸get_candidate_reposç›¸åŒæ ¼å¼çš„æ•°æ®
        return {
            'full_name': repo_data['full_name'],
            'stargazers_count': repo_data['stargazers_count'],
            'size': repo_data['size'],
            'topics': repo_data.get('topics', []),
            'language': repo_data.get('language', ''),
            'archived': repo_data.get('archived', False),
            'disabled': repo_data.get('disabled', False),
            'fork': repo_data.get('fork', False),
        }
        
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 404:
            print(f"  âš ï¸ ä»“åº“ä¸å­˜åœ¨: {repo_name}")
        else:
            print(f"  âš ï¸ è·å–ä»“åº“ä¿¡æ¯å¤±è´¥: {repo_name} - {e}")
        return None
    except Exception as e:
        print(f"  âš ï¸ è·å–ä»“åº“ä¿¡æ¯å¼‚å¸¸: {repo_name} - {e}")
        return None

def get_repositories_to_process(use_cache: bool = True, crawl_mode: str = None) -> Tuple[List[Dict], Dict[str, Repository]]:
    """è·å–éœ€è¦å¤„ç†çš„ä»“åº“åˆ—è¡¨å’Œå·²å¤„ç†çš„ä»“åº“"""
    # åŠ è½½å·²å¤„ç†çš„ä»“åº“ç¼“å­˜
    processed_repos = load_processed_repos() if use_cache else {}
    
    # ç¡®å®šçˆ¬å–æ¨¡å¼
    mode = crawl_mode or CRAWL_MODE
    
    # æ ¹æ®æ¨¡å¼è·å–å€™é€‰ä»“åº“
    if mode == "specified":
        print("ğŸ¯ ä½¿ç”¨æŒ‡å®šä»“åº“æ¨¡å¼")
        candidate_repos = get_specified_repos()
    else:
        print("â­ ä½¿ç”¨æŒ‰staræ•°ç­›é€‰æ¨¡å¼")
        candidate_repos = get_candidate_repos()
    
    if not candidate_repos:
        return [], processed_repos

    # è¿‡æ»¤æ‰å·²å¤„ç†çš„ä»“åº“
    if processed_repos:
        unprocessed_repos = [repo for repo in candidate_repos if repo['full_name'] not in processed_repos]
        candidate_repos = unprocessed_repos

    # åˆæ­¥ç­›é€‰
    pre_filtered_repos = filter_by_metadata_and_releases(candidate_repos)
    
    return pre_filtered_repos, processed_repos